{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a new Model.\n",
    "\n",
    "We have annotated 500 example with 6 labels in our dataset , it is now time to train it and we will use this model to build the script for the training model.\n",
    "\n",
    "The plan is to train a custom model for NER, then use prodigy for with that custom model and reannodate the data with the model, and keep training the model with the new anodated data from pretraining.\n",
    "\n",
    "- https://www.freecodecamp.org/news/getting-started-with-ner-models-using-huggingface/\n",
    "-  And we will be using the Jean-Baptiste/camembert-ner-with-dates model from huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data.\n",
    "\n",
    "We have annodated data from prodigy , we will be using a script to load the data  before training our model on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import PRODIGY_DATABASE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "result = urlparse(PRODIGY_DATABASE_URL)\n",
    "username = result.username\n",
    "password = result.password\n",
    "database = result.path[1:]\n",
    "hostname = result.hostname\n",
    "port = result.port\n",
    "connection_params = {\n",
    "    \"database\": database,\n",
    "    \"user\": username,\n",
    "    \"password\": password,\n",
    "    \"host\": hostname,\n",
    "    \"port\": port\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prodigy.components.db import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_connection = connect(db_id=\"postgresql\", db_settings=connection_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner_cong_news',\n",
       " 'ner_congo_news_log_org_pers_reanodate',\n",
       " 'sample_news_600',\n",
       " 'sample-news-with-bert']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_connection.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "drc_new_annotation = database_connection.get_dataset(\"sample_news_600\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.training import biluo_tags_to_offsets, iob_to_biluo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code convert the data from prodigy anndoation to the data we can use to train a transtofrmer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "examples = ((eg[\"text\"], eg) for eg in drc_new_annotation)\n",
    "for doc, eg in nlp.pipe(examples, as_tuples=True):\n",
    "    if eg.get(\"spans\"):\n",
    "        doc.ents = [doc.char_span(s[\"start\"], s[\"end\"], s[\"label\"]) for s in eg[\"spans\"]]\n",
    "    else :\n",
    "        doc.ents = []\n",
    "    iob_tags = [f\"{t.ent_iob_}-{t.ent_type_}\" if t.ent_iob_ else \"O\" for t in doc]\n",
    "    iob_tags = [t.strip(\"-\") for t in iob_tags]\n",
    "    tokens = [str(t) for t in doc]\n",
    "    biluo_tags = iob_to_biluo(iob_tags)\n",
    "    offsets = biluo_tags_to_offsets(doc, biluo_tags)\n",
    "    temp_data = {\n",
    "        \"tokens\": tokens,\n",
    "        \"tags\": iob_tags,\n",
    "        \"offsets\": offsets,\n",
    "        \"bilod_tags\": biluo_tags,\n",
    "    }\n",
    "    dataset.append(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Il',\n",
       "  'l’',\n",
       "  'a',\n",
       "  'dit',\n",
       "  ',',\n",
       "  'samedi',\n",
       "  '26',\n",
       "  'mars',\n",
       "  ',',\n",
       "  'apres',\n",
       "  'son',\n",
       "  'election',\n",
       "  ',',\n",
       "  'en',\n",
       "  'presence',\n",
       "  'des',\n",
       "  'deputes',\n",
       "  'provinciaux',\n",
       "  'et',\n",
       "  'de',\n",
       "  'nombreuses',\n",
       "  'autres',\n",
       "  'personnalites',\n",
       "  ',',\n",
       "  'a',\n",
       "  'Lubumbashi',\n",
       "  '.',\n",
       "  '«A',\n",
       "  'la',\n",
       "  'naissance',\n",
       "  ',',\n",
       "  'personne',\n",
       "  'n’',\n",
       "  'est',\n",
       "  'venu',\n",
       "  'avec',\n",
       "  'de',\n",
       "  'l’',\n",
       "  'argent',\n",
       "  '.'],\n",
       " 'tags': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-TIME',\n",
       "  'I-TIME',\n",
       "  'I-TIME',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " 'offsets': [(12, 26, 'TIME'), (125, 135, 'LOC')],\n",
       " 'bilod_tags': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-TIME',\n",
       "  'I-TIME',\n",
       "  'L-TIME',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'U-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O']}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"\"\"\n",
    "Mais, retenez que l’essentiel de mon programme est le travail», a indique Jean-Claude Kazembe.Le gouverneur du Haut-Katanga a egalement promis de de travailler en collaboration avec la population pour developper cette  nouvelle province.Jean-Claude Kazembe Musonda a remercie les deputes provinciaux pour la confiance lui a accordee, mais surtout pour leur respect envers la population et l’autorite morale de la Majorite presidentielle (MP), Joseph Kabila :«Le depute elu par la population respecte le choix de ses electeurs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner-with-dates\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner-with-dates\")\n",
    "\n",
    "\n",
    "##### Process text sample (from wikipedia)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"\"\"\n",
    "Le Tout Puissant Mazembe a battu le Football club Lubumbashi-Sport par un score de 3 buts a 1 au stade TP Mazembe de la commune de Kamalondo a Lubumbashi dans le Haut-Katanga, ce mercredi 17 fevrier 2021.C'etait en match de la phase retour de la Ligue Nationale de Football, Vodacom Ligue 1.C'est a la 16eme minute de jeu que les kamikazes ont ouvert le score sur un coup-franc tire par le joueur Wandenga de Lubumbashi-Sport.Moins d'un quart d'heure apres ce but de Lubumbashi-Sport, les corbeaux de Mazembe vont obtenir un penalty a la 30eme minute qui sera transforme par Christian Kouame apres une faute de Kamikazes.Les deux equipes se sont separees a la mi-temps par un but partout.De retour des vestiaires, Adam Bossu va alourdir le score pour le TP Mazembe a la 53eme minute.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "transformed_data = nlp(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.90294963,\n",
       "  'word': 'Tout Puissant Mazembe',\n",
       "  'start': 3,\n",
       "  'end': 25},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9533042,\n",
       "  'word': 'Football club Lubumbashi-Sport',\n",
       "  'start': 36,\n",
       "  'end': 67},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9589909,\n",
       "  'word': 'stade TP Mazembe',\n",
       "  'start': 97,\n",
       "  'end': 114},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9939287,\n",
       "  'word': 'Kamalondo',\n",
       "  'start': 131,\n",
       "  'end': 141},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9953526,\n",
       "  'word': 'Lubumbashi',\n",
       "  'start': 143,\n",
       "  'end': 154},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99548435,\n",
       "  'word': 'Haut-Katanga',\n",
       "  'start': 162,\n",
       "  'end': 175},\n",
       " {'entity_group': 'DATE',\n",
       "  'score': 0.97593147,\n",
       "  'word': 'ce mercredi 17 fevrier 2021',\n",
       "  'start': 176,\n",
       "  'end': 204},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.6167741,\n",
       "  'word': 'Ligue Nationale de Football',\n",
       "  'start': 246,\n",
       "  'end': 274},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.62800646,\n",
       "  'word': 'Vodacom Ligue 1.',\n",
       "  'start': 275,\n",
       "  'end': 292},\n",
       " {'entity_group': 'DATE',\n",
       "  'score': 0.72576076,\n",
       "  'word': '16eme minute',\n",
       "  'start': 302,\n",
       "  'end': 315},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.9744728,\n",
       "  'word': 'Wandenga',\n",
       "  'start': 397,\n",
       "  'end': 406},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.95670754,\n",
       "  'word': 'Lubumbashi-Sport',\n",
       "  'start': 409,\n",
       "  'end': 426},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9478779,\n",
       "  'word': 'Lubumbashi-Sport',\n",
       "  'start': 467,\n",
       "  'end': 484},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.6324959,\n",
       "  'word': 'Mazemb',\n",
       "  'start': 501,\n",
       "  'end': 508},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.55160934,\n",
       "  'word': 'e',\n",
       "  'start': 508,\n",
       "  'end': 509},\n",
       " {'entity_group': 'DATE',\n",
       "  'score': 0.9449231,\n",
       "  'word': '30eme minute',\n",
       "  'start': 538,\n",
       "  'end': 551},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.99531907,\n",
       "  'word': 'Christian Kouame',\n",
       "  'start': 575,\n",
       "  'end': 592},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.5163517,\n",
       "  'word': 'Kam',\n",
       "  'start': 611,\n",
       "  'end': 615},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.54323065,\n",
       "  'word': 'ikazes',\n",
       "  'start': 615,\n",
       "  'end': 621},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.996012,\n",
       "  'word': 'Adam Bossu',\n",
       "  'start': 714,\n",
       "  'end': 725},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.93451613,\n",
       "  'word': 'TP Mazembe',\n",
       "  'start': 754,\n",
       "  'end': 765},\n",
       " {'entity_group': 'DATE',\n",
       "  'score': 0.9453693,\n",
       "  'word': '53eme minute',\n",
       "  'start': 770,\n",
       "  'end': 783}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data.reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 54, 543, 4804, 2914, 16996, 6614, 35, 33, 9252, 16, 12079, 1046, 71, 2509, 19557, 10, 1895, 26, 21154, 37, 23, 3856, 8, 135, 5316, 33, 124, 36, 2623, 18510, 16996, 6614, 35, 8, 13, 928, 8, 11487, 341, 88, 1233, 33, 71, 2509, 19557, 10, 1895, 29, 16, 4860, 26, 707, 622, 16310, 7, 44, 1600, 458, 4152, 16441, 325, 2139, 9, 228, 11, 12821, 22, 1136, 8, 13, 2231, 665, 8, 13, 3593, 4395, 8, 12079, 7, 457, 11808, 310, 3593, 2758, 228, 11, 41, 33, 13, 475, 3141, 3417, 8, 357, 27, 19, 1482, 2900, 1469, 138, 80, 96, 1422, 16, 3856, 32, 23, 392, 26, 16488, 4808, 37, 16, 1954, 692, 8504, 21255, 8, 71, 2509, 19557, 10, 1895, 26, 21154, 9, 7040, 1210, 18, 11, 59, 5207, 18, 11, 1130, 7129, 44, 635, 8, 71, 2509, 19557, 10, 1895, 26, 21154, 7, 19, 27431, 290, 8, 16996, 6614, 35, 774, 1207, 23, 27489, 33, 13, 417, 3141, 3417, 31, 210, 7196, 37, 4012, 16693, 3188, 7129, 28, 2871, 8, 11487, 8250, 138, 80, 9, 1607, 116, 20998, 10, 48, 56, 48, 1244, 35, 80, 33, 13, 1212, 26, 4677, 37, 23, 635, 1397, 9, 4217, 665, 20, 14158, 10, 7, 8681, 22100, 518, 198, 33, 28348, 764, 16, 3856, 24, 16, 18510, 16996, 6614, 35, 33, 13, 6895, 3141, 3417, 9, 21, 6]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Jean-Baptiste/camembert-ner-with-dates were not used when initializing CamembertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertModel were not initialized from the model checkpoint at Jean-Baptiste/camembert-ner-with-dates and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<thinc.optimizers.Optimizer at 0x16ee9a5d0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"fr\") \n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"@architectures\": \"spacy-transformers.TransformerModel.v3\",\n",
    "        \"name\": \"Jean-Baptiste/camembert-ner-with-dates\",\n",
    "        \"tokenizer_config\" : {\"use_fast\": False}\n",
    "    }\n",
    "}\n",
    "nlp.add_pipe(\"transformer\", config=config)\n",
    "nlp.initialize() # XXX don't forget this step!# all the Transformer output is stored here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we have or dataset with tokens and their corresponding tags , now we can think about building a dataset we will be using to train our ner model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Le Tout Puissant Mazembe a battu le Football club Lubumbashi-Sport par un score de 3 buts a 1 au stade TP Mazembe de la commune de Kamalondo a Lubumbashi dans le Haut-Katanga, ce mercredi 17 fevrier 2021.C'etait en match de la phase retour de la Ligue Nationale de Football, Vodacom Ligue 1.C'est a la 16eme minute de jeu que les kamikazes ont ouvert le score sur un coup-franc tire par le joueur Wandenga de Lubumbashi-Sport.Moins d'un quart d'heure apres ce but de Lubumbashi-Sport, les corbeaux de Mazembe vont obtenir un penalty a la 30eme minute qui sera transforme par Christian Kouame apres une faute de Kamikazes.Les deux equipes se sont separees a la mi-temps par un but partout.De retour des vestiaires, Adam Bossu va alourdir le score pour le TP Mazembe a la 53eme minute."
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(text_data).t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/es.py/Library/Caches/pypoetry/virtualenvs/deep-learning-nlp-stuff/lib/python3.7/site-packages/spacy/displacy/__init__.py:200: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>Le Tout Puissant Mazembe a battu le Football club Lubumbashi-Sport par un score de 3 buts a 1 au stade TP Mazembe de la commune de Kamalondo a Lubumbashi dans le Haut-Katanga, ce mercredi 17 fevrier 2021.C'etait en match de la phase retour de la Ligue Nationale de Football, Vodacom Ligue 1.C'est a la 16eme minute de jeu que les kamikazes ont ouvert le score sur un coup-franc tire par le joueur Wandenga de Lubumbashi-Sport.Moins d'un quart d'heure apres ce but de Lubumbashi-Sport, les corbeaux de Mazembe vont obtenir un penalty a la 30eme minute qui sera transforme par Christian Kouame apres une faute de Kamikazes.Les deux equipes se sont separees a la mi-temps par un but partout.De retour des vestiaires, Adam Bossu va alourdir le score pour le TP Mazembe a la 53eme minute.</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(nlp(text_data),  style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is an adaptation from the NER dataset from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 747/747 [00:00<00:00, 292kB/s]\n",
      "Downloading: 100%|██████████| 878k/878k [00:00<00:00, 903kB/s] \n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 634kB/s] \n",
      "Downloading: 100%|██████████| 150/150 [00:00<00:00, 85.6kB/s]\n",
      "Downloading: 100%|██████████| 476M/476M [00:59<00:00, 8.44MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerData(wordpieces=WordpieceBatch(strings=[['<s>', 'sp', 'a', 'Cy', 'Ġis', 'Ġa', 'Ġwonderful', 'Ġtool', '</s>']], input_ids=array([[    0,  4182,   102, 25826,    16,    10,  4613,  3944,     2]],\n",
      "      dtype=int32), attention_mask=array([[1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32), lengths=[9], token_type_ids=None), model_output=ModelOutput([('logits', array([[-2.376998 , -0.316459 ,  3.2421818]], dtype=float32))]), align=Ragged(data=array([[1],\n",
      "       [2],\n",
      "       [3],\n",
      "       [4],\n",
      "       [5],\n",
      "       [6],\n",
      "       [7]], dtype=int32), lengths=array([3, 1, 1, 1, 1], dtype=int32), data_shape=(-1,), starts_ends=None))\n",
      "positive\n",
      "{'prob': array([0.004, 0.028, 0.969], dtype=float32), 'labels': ['negative', 'neutral', 'positive']}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import spacy_wrap\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "config = {\n",
    "    \"doc_extension_trf_data\": \"clf_trf_data\",  # document extention for the forward pass\n",
    "    \"doc_extension_prediction\": \"sentiment\",  # document extention for the prediction\n",
    "    \"labels\": [\"negative\", \"neutral\", \"positive\"],\n",
    "    \"model\": {\n",
    "        \"name\": \"cardiffnlp/twitter-roberta-base-sentiment\",  # the model name or path of huggingface model\n",
    "    },\n",
    "}\n",
    "\n",
    "transformer = nlp.add_pipe(\"classification_transformer\", config=config)\n",
    "\n",
    "doc = nlp(\"spaCy is a wonderful tool\")\n",
    "\n",
    "print(doc._.clf_trf_data)\n",
    "# TransformerData(wordpieces=...\n",
    "print(doc._.sentiment)\n",
    "# 'positive'\n",
    "print(doc._.sentiment_prob)\n",
    "#{'prob': array([0.004, 0.028, 0.969], dtype=float32), 'labels': ['negative', 'neutral', 'positive']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_transformers.span_getters import get_doc_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy_wrap\n",
    "\n",
    "nlp = spacy.blank(\"fr\")\n",
    "\n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"@architectures\": \"spacy-transformers.TransformerModel.v3\",\n",
    "        \"name\": \"Jean-Baptiste/camembert-ner-with-dates\",\n",
    "        \"tokenizer_config\" : {\"use_fast\": False},\n",
    "        \"get_spans\":  {\"@span_getters\": \"spacy-transformers.doc_spans.v1\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "transformer = nlp.add_pipe(\"ner\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'resize_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5_/n81dq93n79l30d_c34cfqxpr0000gn/T/ipykernel_4740/1729775852.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deep-learning-nlp-stuff/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, get_examples, sgd)\u001b[0m\n\u001b[1;32m   1306\u001b[0m                     \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"components\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m                 )\n\u001b[0;32m-> 1308\u001b[0;31m                 \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mp_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m         \u001b[0mpretrain_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pretraining\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpretrain_cfg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deep-learning-nlp-stuff/lib/python3.7/site-packages/spacy/pipeline/transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.initialize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deep-learning-nlp-stuff/lib/python3.7/site-packages/spacy/pipeline/transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser._resize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'resize_output'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon checking I found that it is nearly not possible to use the spacy teacher feature with a tranformer model such as the BERT model we find fined tuned on the Camembert Model. A solution will to use only the tokenization feature offer by the model and annotate a few samples with the pretrain it with those few sample if we are not happy with the results . But as of now , I can see that the model update with the french wikipedia is doing a good job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This failled , we will leverage the cambert model for ner to train our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c2668786ae4e4c4fbfa9e5bd2c1f84381eb94ad61006e099ebff41408861387"
  },
  "kernelspec": {
   "display_name": "nlp-stuff",
   "language": "python",
   "name": "nlp-stuff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
